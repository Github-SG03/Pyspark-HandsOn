{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac77b2ae",
   "metadata": {},
   "source": [
    "FULL STRUCTURED ETL PIPELINE (Databricks Free Compatible)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df09df1",
   "metadata": {},
   "source": [
    "üîπ STEP 1 ‚Äî Initialize Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f774f4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ================================\n",
    "# STEP 1: Initialize Environment\n",
    "# ================================\n",
    "import os\n",
    "import shutil\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, col\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType,\n",
    "    DoubleType\n",
    ")\n",
    "\n",
    "# Start Spark Session\n",
    "spark = SparkSession.builder.appName(\"RetailSalesPipeline\").getOrCreate()   # type: ignore\n",
    "\n",
    "\n",
    "# Unity Catalog Volume Info\n",
    "CATALOG = \"workspace\"\n",
    "SCHEMA = \"default\"\n",
    "VOLUME = \"my_etl_data\"\n",
    "VOLUME_PATH = f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}\"\n",
    "\n",
    "\n",
    "# Create Volume (if not exists)\n",
    "spark.sql(f\"\"\"CREATE VOLUME IF NOT EXISTS {CATALOG}.{SCHEMA}.{VOLUME}\"\"\")\n",
    "\n",
    "print(\"‚úÖ Storage Volume Ready at:\")\n",
    "print(VOLUME_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0439c634",
   "metadata": {},
   "source": [
    "üîπ STEP 2 ‚Äî Data Bridge (Extract ‚Üí Volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cd5646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# STEP 2: Data Bridge (Extract)\n",
    "# ================================\n",
    "\n",
    "# Current working directory (inside Databricks job)\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "\n",
    "# Expected bundle path\n",
    "local_sync_path = os.path.join(\n",
    "    os.path.dirname(current_dir),\n",
    "    \"datasets\",\n",
    "    \"2015-summary.csv\"\n",
    ")\n",
    "\n",
    "\n",
    "# Target Volume Path\n",
    "target_volume_file = f\"{VOLUME_PATH}/2015-summary.csv\"\n",
    "\n",
    "\n",
    "print(\"üìÇ Source File:\", local_sync_path)\n",
    "print(\"üì¶ Target File:\", target_volume_file)\n",
    "\n",
    "\n",
    "try:\n",
    "\n",
    "    # Create folder if missing\n",
    "    os.makedirs(os.path.dirname(target_volume_file), exist_ok=True)\n",
    "\n",
    "    # Copy file to Volume\n",
    "    shutil.copy(local_sync_path, target_volume_file)\n",
    "\n",
    "    print(\"‚úÖ File copied to Volume successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "\n",
    "    print(\"‚ö†Ô∏è Auto copy failed.\")\n",
    "    print(\"üëâ Please upload file manually to Volume/input\")\n",
    "    print(\"Error:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017dae33",
   "metadata": {},
   "source": [
    "üîπ STEP 3 ‚Äî Read & Transform (Cleanse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c799d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# STEP 3: Read & Transform\n",
    "# ================================\n",
    "\n",
    "\n",
    "# Read CSV from Volume\n",
    "raw_sales_df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(target_volume_file)\n",
    "\n",
    "\n",
    "print(\"üìã Raw Data Preview:\")\n",
    "raw_sales_df.show(5)\n",
    "\n",
    "\n",
    "# Data Cleansing\n",
    "cleansed_sales_df = raw_sales_df \\\n",
    "    .withColumn(\"ingestion_date\", lit(\"2026-02-10\")) \\\n",
    "    .filter(col(\"TOTAL_COUNT\").isNotNull())\n",
    "\n",
    "\n",
    "print(\"‚ú® Cleansing Completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75bba4b",
   "metadata": {},
   "source": [
    "üîπ STEP 4 ‚Äî Load (Silver Layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b526c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# STEP 4: Load (Silver Layer)\n",
    "# ================================\n",
    "\n",
    "\n",
    "# Output Path\n",
    "output_path = f\"{VOLUME_PATH}/silver_sales_data\"\n",
    "\n",
    "\n",
    "try:\n",
    "\n",
    "    # Write Parquet\n",
    "    cleansed_sales_df \\\n",
    "        .write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .parquet(output_path)\n",
    "\n",
    "\n",
    "    print(\"üéâ JOB SUCCESS\")\n",
    "    print(\"Saved at:\", output_path)\n",
    "\n",
    "\n",
    "    # Verify\n",
    "    print(\"üìà Verification Preview:\")\n",
    "\n",
    "    spark.read \\\n",
    "        .parquet(output_path) \\\n",
    "        .select(\n",
    "            \"COUNTRY_1\",\n",
    "            \"TOTAL_COUNT\",\n",
    "            \"ingestion_date\"\n",
    "        ) \\\n",
    "        .show(5)\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "\n",
    "    print(\"‚ùå Load Failed\")\n",
    "    print(\"Error:\", e)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
