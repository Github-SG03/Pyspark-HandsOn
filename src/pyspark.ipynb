{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8503e16",
   "metadata": {},
   "source": [
    "Step 1: ‚öôÔ∏è Initialize Environment & Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22323b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.!/usr/bin/env python3\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import * #type: ignore\n",
    "from pyspark.sql.window import Window #type: ignore\n",
    "from pyspark.sql.types import * #type: ignore\n",
    "import os\n",
    "import json\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"MyPySparkAutomatic\").getOrCreate() #type: ignore\n",
    "\n",
    "# 1. Create Volume for ETL data\n",
    "CATALOG = \"workspace\"\n",
    "SCHEMA = \"default\"\n",
    "VOLUME = \"my_elt_data\"\n",
    "VOLUME_PATH = f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}\"\n",
    "\n",
    "# Create Volume if it doesn't exist \n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {CATALOG}.{SCHEMA}.{VOLUME}\")\n",
    "\n",
    "# 2. Prepare directories\n",
    "INPUT_VOL = f\"{VOLUME_PATH}/input\"\n",
    "OUTPUT_VOL = f\"{VOLUME_PATH}/output\"\n",
    "OTHER_VOL = f\"{VOLUME_PATH}/other\"\n",
    "\n",
    "# USE DBUTILS for Volumes - this is the \"Databricks Way\"\n",
    "for folder in [INPUT_VOL, OUTPUT_VOL, OTHER_VOL]:\n",
    "    dbutils.fs.mkdirs(folder)  #type: ignore\n",
    "\n",
    "print(\"‚úÖ Volume and Folders ready:\", VOLUME_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4595c9c",
   "metadata": {},
   "source": [
    "Step 2: üåâ The Data Bridge (Extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef43199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# STEP 2: Data Bridge (Extract) - SHUTIL FIX\n",
    "# =====================================================\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# 1. Correct paths based on your bundle structure\n",
    "# The error shows your files are at: /Workspace/Users/.../.bundle/default/dev/files/datasets/input/\n",
    "current_dir = os.getcwd() # e.g., .../files/scripts\n",
    "parent_dir = os.path.dirname(current_dir) # e.g., .../files\n",
    "\n",
    "# Your synced data source\n",
    "local_source_file = os.path.join(parent_dir, \"datasets\", \"input\", \"2015-summary.csv\")\n",
    "\n",
    "# Your target Volume destination\n",
    "target_volume_file = f\"{INPUT_VOL}/2015-summary.csv\"\n",
    "\n",
    "print(f\"üìÇ Source: {local_source_file}\")\n",
    "print(f\"üì¶ Target: {target_volume_file}\")\n",
    "\n",
    "try:\n",
    "    # Ensure the target directory in the Volume exists\n",
    "    os.makedirs(os.path.dirname(target_volume_file), exist_ok=True)\n",
    "    \n",
    "    # Use SHUTIL to copy (Avoids dbutils SecurityException)\n",
    "    shutil.copy(local_source_file, target_volume_file)\n",
    "    \n",
    "    print(\"‚úÖ SUCCESS: File copied to Volume using shutil!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"üí° If it fails, manually verify if the source exists locally using os.path.exists\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33d639b",
   "metadata": {},
   "source": [
    "Step 3: ‚ú® Read CSV (AUTO SCHEMA) (Cleanse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b22da92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CSV_PATH= target_volume_file\n",
    "\n",
    "flight_df_1 = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(CSV_PATH)\n",
    "\n",
    "flight_df_1.show()\n",
    "flight_df_1.printSchema()\n",
    "print(\"Total Rows:\", flight_df_1.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6403c6d",
   "metadata": {},
   "source": [
    "Step 4: Manual Schema + Bad Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2794cede",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "my_schema = StructType([\n",
    "    StructField(\"COUNTRY_1\", StringType()),\n",
    "    StructField(\"COUNTRY_2\", StringType()),\n",
    "    StructField(\"TOTAL_COUNT\", IntegerType())\n",
    "])\n",
    "\n",
    "flight_df_2 = spark.read \\\n",
    "    .schema(my_schema) \\\n",
    "    .option(\"mode\", \"PERMISSIVE\") \\\n",
    "    .csv(CSV_PATH)\n",
    "\n",
    "bad_df = flight_df_2.filter(col(\"TOTAL_COUNT\").isNull())\n",
    "\n",
    "bad_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2820872a",
   "metadata": {},
   "source": [
    "STEP 5 ‚Äî Read JSON (FROM Volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce720c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "JSON_PATH = f\"{target_path}/multiline.json\"\n",
    "\n",
    "people_df = spark.read \\\n",
    "    .option(\"multiline\", \"true\") \\\n",
    "    .json(JSON_PATH)\n",
    "\n",
    "people_df.show()\n",
    "people_df.printSchema()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2037bcd2",
   "metadata": {},
   "source": [
    "STEP 6 ‚Äî Parquet (Volume-safe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf98427b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARQUET_PATH = f\"{OUTPUT_VOL}/parquet_data\"\n",
    "\n",
    "flight_df_1.write.mode(\"overwrite\").parquet(PARQUET_PATH)\n",
    "\n",
    "parquet_df = spark.read.parquet(PARQUET_PATH)\n",
    "parquet_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c866672",
   "metadata": {},
   "source": [
    "STEP 7 ‚Äî CSV Output (Volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579c4ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_OUT = f\"{OUTPUT_VOL}/csv_output\"\n",
    "\n",
    "parquet_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(CSV_OUT)\n",
    "\n",
    "print(\"‚úÖ CSV written to Volume\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a955939c",
   "metadata": {},
   "source": [
    "STEP 8 ‚Äî Partitioned CSV (FIXED COLUMN NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa39455b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARTITION_OUT = f\"{OUTPUT_VOL}/partitioned_csv\"\n",
    "\n",
    "parquet_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .partitionBy(\"COUNTRY_1\") \\\n",
    "    .csv(PARTITION_OUT)\n",
    "\n",
    "print(\"‚úÖ Partitioned output ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12675958",
   "metadata": {},
   "source": [
    "STEP 9 ‚Äî Data Engineering Pipeline (READ ‚Üí TRANSFORM ‚Üí WRITE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ae8d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 9.0Create a DataFrame using the DataFrame API for pe4rforming transformations in PySpark\n",
    "data = [(1, 1),(2, 1),(3, 1),(4, 2),(5, 1),(6, 2),(7, 2)]\n",
    "columns = [\"id\", \"num\"]\n",
    "example_df = spark.createDataFrame(data, columns)\n",
    "print(\"‚úÖ Example DataFrame Created\")\n",
    "example_df.show()\n",
    "example_df.printSchema()\n",
    "\n",
    "#STEP 9.1:Transformation in PySpark:Using Select Method\n",
    "emp_data = [\n",
    "    (1,\"Amit\",70000,\"M\",\"INDIA\",\"Delhi\",25),\n",
    "    (2,\"Neha\",80000,\"F\",\"JAPAN\",\"Tokyo\",28),\n",
    "    (3,\"Raj\",60000,\"M\",\"INDIA\",\"Mumbai\",30),\n",
    "    (4,\"Sara\",90000,\"F\",\"JAPAN\",\"Osaka\",26),\n",
    "    (5,\"Tom\",75000,\"M\",\"USA\",\"NY\",35)]\n",
    "emp_schema = [\"id\",\"name\",\"salary\",\"gender\",\"address\",\"city\",\"age\"]\n",
    "employee_df = spark.createDataFrame(emp_data, emp_schema)\n",
    "# Create a Transformation using the DataFrame API & storing in another DataFrame as variable\n",
    "employee_df_1 = employee_df.select(\"id\", \"salary\", (col(\"id\") + 5).alias(\"id_plus_5\"), employee_df.gender, employee_df[\"address\"])\n",
    "employee_df_2 = employee_df.select(expr(\"id+5\").alias(\"id_plus_5\"), expr(\"salary*2\").alias(\"salary_times_2\"), expr(\"concat(name, address)\").alias(\"name_address\"))\n",
    "#show the DataFrame\n",
    "employee_df_1.show(truncate=False)\n",
    "employee_df_2.show(truncate=False)\n",
    "\n",
    "#STEP 9.2:Transformation in PySpark:Using Spark SQL(select query)\n",
    "#Crating a temporary view for the DataFrame\n",
    "employee_df.createOrReplaceTempView(\"employee_tbl\")\n",
    "# Create a SQL query to select the desired columns\n",
    "employee_tbl_1=spark.sql(\"\"\"select * from employee_tbl where salary > 70000\"\"\")\n",
    "employee_tbl_1.show(truncate=False)\n",
    "\n",
    "#STEP 9.3:Transformation in PySpark:Using dataframe API Filter,Aliases,Literal,Casting,etc\n",
    "#Alises the DataFrame\n",
    "employee_df_4 = employee_df.select(\"id\", \"salary\", (col(\"id\") + 5).alias(\"id_plus_5\"), employee_df.gender, employee_df[\"address\"])\n",
    "# Filtering the DataFrame using the DataFrame API\n",
    "employee_df_5 = employee_df.filter(col(\"address\") == \"JAPAN\")\n",
    "employee_df_6 = employee_df.filter((col(\"address\") == \"JAPAN\") & (col(\"salary\") > 70000)) \\\n",
    "    .select(\"id\", \"salary\", (col(\"id\") + 5).alias(\"id_plus_5\"))\n",
    "employee_df_7 = employee_df.select(\"id\", \"salary\", (col(\"id\") + 5).alias(\"id_plus_5\")).where(\"address = 'JAPAN' and salary > 70000\")\n",
    "#Literal function used to create a column with a constant value\n",
    "employee_df_8 = employee_df.select(\"*\", lit(\"Gupta\").alias(\"last_name\"))\n",
    "employee_df_9= employee_df.withColumn(\"last_name\", lit(\"Gupta\"))\n",
    "#Renaming the columns\n",
    "employee_df_10 = (\n",
    "    employee_df.withColumnRenamed(\"id\", \"emp_id\")\n",
    "    .withColumnRenamed(\"salary\", \"emp_salary\")\n",
    "    .withColumnRenamed(\"address\", \"emp_address\")\n",
    "    .withColumnRenamed(\"gender\", \"emp_gender\")\n",
    "    .withColumnRenamed(\"name\", \"emp_name\")\n",
    "    .withColumnRenamed(\"last_name\", \"emp_last_name\")\n",
    "    .withColumnRenamed(\"age\", \"emp_age\"))\n",
    "#Casting the column\n",
    "employee_df_11 = employee_df.withColumn(\"id\", col(\"id\").cast(StringType())).withColumn(\"salary\", col(\"salary\").cast(\"long\"))\n",
    "#Dropping the column\n",
    "employee_df_12 = employee_df.drop(\"last_name\", \"age\", \"address\", \"gender\", \"name\", \"id\",)\n",
    "# Show the DataFrame\n",
    "employee_df_4.show(truncate=False)\n",
    "employee_df_5.show(truncate=False)\n",
    "employee_df_6.show(truncate=False)\n",
    "employee_df_7.show(truncate=False)\n",
    "employee_df_8.show(truncate=False)\n",
    "employee_df_9.show(truncate=False)\n",
    "employee_df_10.show(truncate=False)\n",
    "employee_df_11.printSchema()\n",
    "employee_df_12.show(truncate=False)\n",
    "employee_df.show(truncate=False)\n",
    "\n",
    "#STEP 9.4:Transformation in PySpark:Using dataframe API Union & Union All(Same in Datafreme API But Different in Spark SQL)\n",
    "#Create a Data for manager1\n",
    "data=[(10 ,'Anil',50000, 18),\n",
    "(11 ,'Vikas',75000,  16),\n",
    "(12 ,'Nisha',40000,  18),\n",
    "(13 ,'Nidhi',60000,  17),\n",
    "(14 ,'Priya',80000,  18),\n",
    "(15 ,'Mohit',45000,  18),\n",
    "(16 ,'Rajesh',90000, 10),\n",
    "(17 ,'Raman',55000, 16),\n",
    "(18 ,'Sam',65000,   17),\n",
    "(18 ,'Sam',65000,   17)]\n",
    "# Create a schema for the DataFrame\n",
    "schema=['id', 'name', 'sal', 'mngr_id']\n",
    "# Create a DataFrame using the DataFrame API\n",
    "manager_df_1 = spark.createDataFrame(data, schema)\n",
    "# Show the DataFrame\n",
    "manager_df_1.show(truncate=False)\n",
    "#show the schema of the DataFrame\n",
    "manager_df_1.printSchema()\n",
    "#show the total number of rows in the DataFrame\n",
    "print(\"Total number of rows in the DataFrame:\", manager_df_1.count())\n",
    "#create data for manager2\n",
    "data1=[(19 ,'Sohan',50000, 18),\n",
    "(20 ,'Sima',75000,  17)]\n",
    "# Create a schema for the DataFrame\n",
    "schema1=['id', 'name', 'sal', 'mngr_id']\n",
    "# Create a DataFrame using the DataFrame API\n",
    "manager_df_2 = spark.createDataFrame(data1, schema1)\n",
    "# Show the DataFrame\n",
    "manager_df_2.show(truncate=False)\n",
    "#show the schema of the DataFrame\n",
    "manager_df_2.printSchema()\n",
    "#show the total number of rows in the DataFrame\n",
    "print(\"Total number of rows in the DataFrame:\", manager_df_2.count())\n",
    "#Union of two DataFrames\n",
    "manager_df_union = manager_df_1.union(manager_df_2)\n",
    "manager_df_unionAll= manager_df_1.unionAll(manager_df_2)\n",
    "manager_df_unionByName= manager_df_1.unionByName(manager_df_2)\n",
    "# Show the DataFrame\n",
    "manager_df_union.show(truncate=False)\n",
    "print(\"Total number of rows in the DataFrame:\", manager_df_union.count())\n",
    "manager_df_unionAll.show(truncate=False)\n",
    "print(\"Total number of rows in the DataFrame:\", manager_df_unionAll.count())\n",
    "manager_df_unionByName.show(truncate=False)\n",
    "print(\"Total number of rows in the DataFrame:\", manager_df_unionByName.count())\n",
    "\n",
    "#STEP 9.5:Transformation in PySpark:Using dataframe API-Case(if-else comaprison using when/otherwise)\n",
    "# Create data for DataFrame\n",
    "emp_data = [\n",
    "(1,'manish',26,20000,'india','IT'),\n",
    "(2,'rahul',None,40000,'germany','engineering'),\n",
    "(3,'pawan',12,60000,'india','sales'),\n",
    "(4,'roshini',44,None,'uk','engineering'),\n",
    "(5,'raushan',35,70000,'india','sales'),\n",
    "(6,None,29,200000,'uk','IT'),\n",
    "(7,'adam',37,65000,'us','IT'),\n",
    "(8,'chris',16,40000,'us','sales'),\n",
    "(None,None,None,None,None,None),\n",
    "(7,'adam',37,65000,'us','IT')]\n",
    "# Create a schema for the DataFrame\n",
    "schema = ['id', 'name', 'age', 'salary', 'address', 'department']\n",
    "# Create a DataFrame using the DataFrame API\n",
    "emp_df= spark.createDataFrame(emp_data, schema)\n",
    "# Show the DataFrame\n",
    "emp_df.show(truncate=False)\n",
    "#show the schema of the DataFrame\n",
    "emp_df.printSchema()\n",
    "#show the total number of rows in the DataFrame\n",
    "print(\"Total number of rows in the DataFrame:\", emp_df.count())\n",
    "#Checking the Age of the employee if they are adult or not(otherwise).Assuming emp_df is your original DataFrame\n",
    "emp_df_1 = emp_df.withColumn(\n",
    "    \"is_adult\",\n",
    "    when(col(\"age\").isNull(), None)           # If age is null ‚Üí null\n",
    "    .when(col(\"age\") > 18, \"Yes\")             # If age > 18 ‚Üí \"Yes\"\n",
    "    .otherwise(\"No\"))                         # Otherwise ‚Üí \"No\")\n",
    "emp_df_2 = emp_df.withColumn(\n",
    "    \"is_adult\",\n",
    "    when((col(\"age\")>0) &(col(\"age\")<18), \"minor\")          \n",
    "    .when((col(\"age\")>18) &(col(\"age\")<30), \"medium\")                        \n",
    "    .otherwise(\"major\"))\n",
    "# Show the DataFrame\n",
    "emp_df_1.show(truncate=False)\n",
    "emp_df_2.show(truncate=False)\n",
    "\n",
    "\n",
    "#STEP 9.6:Transformation in PySpark:Using dataframe API-Case(Unique & Sorted Record in datafarame)\n",
    "# Create data for DataFrame\n",
    "data=[(10 ,'Anil',50000, 18),\n",
    "(11 ,'Vikas',75000,  16),\n",
    "(12 ,'Nisha',40000,  18),\n",
    "(13 ,'Nidhi',60000,  17),\n",
    "(14 ,'Priya',80000,  18),\n",
    "(15 ,'Mohit',45000,  18),\n",
    "(16 ,'Rajesh',90000, 10),\n",
    "(17 ,'Raman',55000, 16),\n",
    "(18 ,'Sam',65000,   17),\n",
    "(15 ,'Mohit',45000,  18),\n",
    "(13 ,'Nidhi',60000,  17),      \n",
    "(14 ,'Priya',90000,  18),  \n",
    "(18 ,'Sam',65000,   17)]\n",
    "# Create a schema for the DataFrame\n",
    "schema = ['id', 'name', 'sal', 'mngr_id']\n",
    "# Create a DataFrame using the DataFrame API\n",
    "mngr_df = spark.createDataFrame(data, schema)\n",
    "# Show the DataFrame\n",
    "mngr_df.show(truncate=False)\n",
    "#show the schema of the DataFrame\n",
    "mngr_df.printSchema()\n",
    "#show the total number of rows in the DataFrame\n",
    "print(\"Total number of rows in the DataFrame:\", mngr_df.count())\n",
    "# Finding unique records & deleting/droping duplicates in the DataFrame\n",
    "mngr_df_1 = mngr_df.distinct()\n",
    "mngr_df_2 = mngr_df.select(\"id\", \"name\").distinct() #selecting distinct records from dataframe created using the id & name columns\n",
    "mngr_df_3 = mngr_df.dropDuplicates([\"id\", \"name\", \"sal\", \"mngr_id\"]) #droping duplicates from the DataFrame using the id & name columns\n",
    "#sorting the DataFrame\n",
    "mngr_df_4 = mngr_df_1.sort(col(\"sal\").desc(),col(\"name\").asc()) #sorting the DataFrame using the sal column\n",
    "#show the schema of the DataFrame\n",
    "mngr_df_1.show(truncate=False)\n",
    "mngr_df_2.show(truncate=False)\n",
    "mngr_df_3.show(truncate=False)\n",
    "mngr_df_4.show(truncate=False)\n",
    "\n",
    "\n",
    "#STEP 9.7:Transformation in PySpark:Using dataframe API-Aggregate function\n",
    "## Create data for DataFrame\n",
    "empl_data = [\n",
    "(1,'manish',26,20000,'india','IT'),\n",
    "(2,'rahul',None,40000,'germany','engineering'),\n",
    "(3,'pawan',12,60000,'india','sales'),\n",
    "(4,'roshini',44,None,'uk','engineering'),\n",
    "(5,'raushan',35,70000,'india','sales'),\n",
    "(6,None,29,200000,'uk','IT'),\n",
    "(7,'adam',37,65000,'us','IT'),\n",
    "(8,'chris',16,40000,'us','sales'),\n",
    "(None,None,None,None,None,None),\n",
    "(7,'adam',37,65000,'us','IT')]\n",
    "# Create a schema for the DataFrame\n",
    "schema = ['id', 'name', 'age', 'salary', 'address', 'department']\n",
    "# Create a DataFrame using the DataFrame API\n",
    "empl_df= spark.createDataFrame(empl_data, schema)\n",
    "# Show the DataFrame\n",
    "empl_df.show(truncate=False)\n",
    "#show the schema of the DataFrame\n",
    "empl_df.printSchema()\n",
    "#show the total number of rows in the DataFrame\n",
    "print(\"Total number of rows in the DataFrame:\", empl_df.count())\n",
    "#count() function is used to count the number of rows in the DataFrame\n",
    "empl_df_1 = empl_df.select(count(\"*\"))\n",
    "empl_df_2 = empl_df.select(count(\"name\")) \n",
    "empl_df_3 = empl_df.select(countDistinct(\"address\").alias(\"distinct_address_count\")) #counting the distinct records in the DataFrame using the address column\n",
    "#min().max() and avg() function is used to find the minimum, maximum and average of the column in the DataFrame\n",
    "empl_df_4 = empl_df.select(min(\"salary\").alias(\"min_salary\"), max(\"salary\").alias(\"max_salary\"), avg(\"salary\").alias(\"avg_salary\")) #finding the min, max and avg of the salary column in the DataFrame\n",
    "#show the DataFrame\n",
    "empl_df_1.show(truncate=False)\n",
    "empl_df_2.show(truncate=False)\n",
    "empl_df_3.show(truncate=False)\n",
    "empl_df_4.show(truncate=False)\n",
    "\n",
    "#STEP 9.8:Transformation in PySpark:Using dataframe API-GroupBy\n",
    "#data for DataFrame\n",
    "data=[(1,'manish',50000,\"IT\"),\n",
    "(2,'vikash',60000,\"sales\"),\n",
    "(3,'raushan',70000,\"marketing\"),\n",
    "(4,'mukesh',80000,\"IT\"),\n",
    "(5,'pritam',90000,\"sales\"),\n",
    "(6,'nikita',45000,\"marketing\"),\n",
    "(7,'ragini',55000,\"marketing\"),\n",
    "(8,'rakesh',100000,\"IT\"),\n",
    "(9,'aditya',65000,\"IT\"),\n",
    "(10,'rahul',50000,\"marketing\")]\n",
    "# Create a schema for the DataFrame\n",
    "schema = ['id', 'name', 'salary', 'department']\n",
    "# Create a DataFrame using the DataFrame API\n",
    "dept_df = spark.createDataFrame(data, schema)\n",
    "# Show the DataFrame\n",
    "dept_df.show(truncate=False)\n",
    "#show the schema of the DataFrame\n",
    "dept_df.printSchema()\n",
    "#show the total number of rows in the DataFrame\n",
    "print(\"Total number of rows in the DataFrame:\", dept_df.count())\n",
    "#groupby() function is used to group the DataFrame by the department column\n",
    "dept_df_2 = dept_df.groupBy(\"department\").agg(count(\"*\").alias(\"count\"), avg(\"salary\").alias(\"avg_salary\"), min(\"salary\").alias(\"min_salary\"), max(\"salary\").alias(\"max_salary\")) #counting the number of records in the DataFrame using the department column and finding the min, max and avg of the salary column in the DataFrame\n",
    "# Show the DataFrame\n",
    "dept_df_2.show(truncate=False)\n",
    "\n",
    "\n",
    "#STEP 9.9:Transformation in PySpark:Using dataframe API-joins\n",
    "#Create 'costomer_data' data for dataframe\n",
    "customer_data = [(1,'manish','patna',\"30-05-2022\"),\n",
    "(2,'vikash','kolkata',\"12-03-2023\"),\n",
    "(3,'nikita','delhi',\"25-06-2023\"),\n",
    "(4,'rahul','ranchi',\"24-03-2023\"),\n",
    "(5,'mahesh','jaipur',\"22-03-2023\"),\n",
    "(6,'prantosh','kolkata',\"18-10-2022\"),\n",
    "(7,'raman','patna',\"30-12-2022\"),\n",
    "(8,'prakash','ranchi',\"24-02-2023\"),\n",
    "(9,'ragini','kolkata',\"03-03-2023\"),\n",
    "(10,'raushan','jaipur',\"05-02-2023\")]\n",
    "# Create a schema for the DataFrame\n",
    "customer_schema=['customer_id','customer_name','address','date_of_joining']\n",
    "# Create a DataFrame using the DataFrame API\n",
    "customer_df = spark.createDataFrame(customer_data, customer_schema)\n",
    "# Show the DataFrame\n",
    "customer_df.show(truncate=False)\n",
    "#show the schema of the DataFrame\n",
    "customer_df.printSchema()\n",
    "#show the total number of rows in the DataFrame\n",
    "print(\"Total number of rows in the DataFrame:\", customer_df.count())\n",
    "# Create a new DataFrame with the date column converted to a date type\n",
    "#Create 'sales_data' data for dataframe\n",
    "sales_data = [(1,22,10,\"01-06-2022\"),\n",
    "(1,27,5,\"03-02-2023\"),\n",
    "(2,5,3,\"01-06-2023\"),\n",
    "(5,22,1,\"22-03-2023\"),\n",
    "(7,22,4,\"03-02-2023\"),\n",
    "(9,5,6,\"03-03-2023\"),\n",
    "(2,1,12,\"15-06-2023\"),\n",
    "(1,56,2,\"25-06-2023\"),\n",
    "(5,12,5,\"15-04-2023\"),\n",
    "(11,12,76,\"12-03-2023\")]\n",
    "# Create a schema for the DataFrame\n",
    "sales_schema=['customer_id','product_id','quantity','date_of_purchase']\n",
    "# Create a DataFrame using the DataFrame API\n",
    "sales_df = spark.createDataFrame(sales_data, sales_schema)\n",
    "# Show the DataFrame\n",
    "sales_df.show(truncate=False)\n",
    "#show the schema of the DataFrame\n",
    "sales_df.printSchema()\n",
    "#show the total number of rows in the DataFrame\n",
    "print(\"Total number of rows in the DataFrame:\", sales_df.count())\n",
    "#Create 'product_data' data for dataframe\n",
    "product_data = [(1, 'fanta',20),\n",
    "(2, 'dew',22),\n",
    "(5, 'sprite',40),\n",
    "(7, 'redbull',100),\n",
    "(12,'mazza',45),\n",
    "(22,'coke',27),\n",
    "(25,'limca',21),\n",
    "(27,'pepsi',14),\n",
    "(56,'sting',10)]\n",
    "# Create a schema for the DataFrame\n",
    "product_schema=['id','name','price']\n",
    "# Create a DataFrame using the DataFrame API\n",
    "product_df = spark.createDataFrame(product_data, product_schema)\n",
    "# Show the DataFrame\n",
    "product_df.show(truncate=False)\n",
    "#show the schema of the DataFrame\n",
    "product_df.printSchema()\n",
    "#show the total number of rows in the DataFrame\n",
    "print(\"Total number of rows in the DataFrame:\", product_df.count())\n",
    "# Inner Join\n",
    "customer_sales_inner_df = customer_df.join(sales_df, customer_df.customer_id == sales_df.customer_id, \"inner\").select(sales_df.product_id).sort(col(\"product_id\").asc())#join on single column\n",
    "# Left Join\n",
    "customer_sales_left_df = customer_df.join(sales_df, customer_df.customer_id == sales_df.customer_id, \"left\").select(sales_df.product_id).sort(col(\"product_id\").asc())#join on single column\n",
    "# Right Join\n",
    "customer_sales_right_df = customer_df.join(sales_df, customer_df.customer_id == sales_df.customer_id, \"right\").select(sales_df.product_id).sort(col(\"product_id\").asc())#join on single column\n",
    "#Outer Join\n",
    "customer_sales_outer_df = customer_df.join(sales_df, customer_df.customer_id == sales_df.customer_id, \"outer\").select(sales_df.product_id).sort(col(\"product_id\").asc())#join on single column\n",
    "# Cross Join\n",
    "customer_sales_cross_df = customer_df.crossJoin(sales_df)\n",
    "# Show the DataFrame\n",
    "customer_sales_inner_df.show(truncate=False)\n",
    "customer_sales_left_df.show(truncate=False)\n",
    "customer_sales_right_df.show(truncate=False)\n",
    "customer_sales_outer_df.show(truncate=False)\n",
    "customer_sales_cross_df.show(truncate=False)\n",
    "\n",
    "#STEP 9.10:Transformation in PySpark:Using dataframe API-Window Function\n",
    "#data for DataFrame\n",
    "e_data = [(1,'manish',50000,'IT','m'),\n",
    "(2,'vikash',60000,'sales','m'),\n",
    "(3,'raushan',70000,'marketing','m'),\n",
    "(4,'mukesh',80000,'IT','m'),\n",
    "(5,'priti',90000,'sales','f'),\n",
    "(6,'nikita',45000,'marketing','f'),\n",
    "(7,'ragini',55000,'marketing','f'),\n",
    "(8,'rashi',100000,'IT','f'),\n",
    "(9,'aditya',65000,'IT','m'),\n",
    "(10,'rahul',50000,'marketing','m'),\n",
    "(11,'rakhi',50000,'IT','f'),\n",
    "(12,'akhilesh',90000,'sales','m')]\n",
    "# Create a schema for the DataFrame\n",
    "schema = ['id', 'name', 'salary', 'department', 'gender']\n",
    "# Create a DataFrame using the DataFrame API\n",
    "e_df = spark.createDataFrame(e_data, schema)\n",
    "# Show the DataFrame\n",
    "e_df.show(truncate=False)\n",
    "#show the schema of the DataFrame\n",
    "e_df.printSchema()\n",
    "#show the total number of rows in the DataFrame\n",
    "print(\"Total number of rows in the DataFrame:\", e_df.count())\n",
    "# Create product_data for DataFrame API\n",
    "product_data = [\n",
    "(1,\"iphone\",\"01-01-2023\",1500000),\n",
    "(2,\"samsung\",\"01-01-2023\",1100000),\n",
    "(3,\"oneplus\",\"01-01-2023\",1100000),\n",
    "(1,\"iphone\",\"01-02-2023\",1300000),\n",
    "(2,\"samsung\",\"01-02-2023\",1120000),\n",
    "(3,\"oneplus\",\"01-02-2023\",1120000),\n",
    "(1,\"iphone\",\"01-03-2023\",1600000),\n",
    "(2,\"samsung\",\"01-03-2023\",1080000),\n",
    "(3,\"oneplus\",\"01-03-2023\",1160000),\n",
    "(1,\"iphone\",\"01-04-2023\",1700000),\n",
    "(2,\"samsung\",\"01-04-2023\",1800000),\n",
    "(3,\"oneplus\",\"01-04-2023\",1170000),\n",
    "(1,\"iphone\",\"01-05-2023\",1200000),\n",
    "(2,\"samsung\",\"01-05-2023\",980000),\n",
    "(3,\"oneplus\",\"01-05-2023\",1175000),\n",
    "(1,\"iphone\",\"01-06-2023\",1100000),\n",
    "(2,\"samsung\",\"01-06-2023\",1100000),\n",
    "(3,\"oneplus\",\"01-06-2023\",1200000)]\n",
    "# Create a schema for the DataFrame\n",
    "product_schema = ['product_id', 'product_name', 'sales_date', 'sales']\n",
    "# Create a DataFrame using the DataFrame API\n",
    "product_df = spark.createDataFrame(product_data, product_schema)\n",
    "# Show the DataFrame\n",
    "product_df.show(truncate=False)\n",
    "#show the schema of the DataFrame\n",
    "product_df.printSchema()\n",
    "#show the total number of rows in the DataFrame\n",
    "print(\"Total number of rows in the DataFrame:\", product_df.count())\n",
    "#create data for DataFrame\n",
    "empls_data = [(1,\"manish\",\"11-07-2023\",\"10:20\"),\n",
    "        (1,\"manish\",\"11-07-2023\",\"11:20\"),\n",
    "        (2,\"rajesh\",\"11-07-2023\",\"11:20\"),\n",
    "        (1,\"manish\",\"11-07-2023\",\"11:50\"),\n",
    "        (2,\"rajesh\",\"11-07-2023\",\"13:20\"),\n",
    "        (1,\"manish\",\"11-07-2023\",\"19:20\"),\n",
    "        (2,\"rajesh\",\"11-07-2023\",\"17:20\"),\n",
    "        (1,\"manish\",\"12-07-2023\",\"10:32\"),\n",
    "        (1,\"manish\",\"12-07-2023\",\"12:20\"),\n",
    "        (3,\"vikash\",\"12-07-2023\",\"09:12\"),\n",
    "        (1,\"manish\",\"12-07-2023\",\"16:23\"),\n",
    "        (3,\"vikash\",\"12-07-2023\",\"18:08\")]\n",
    "# Create a schema for the DataFrame\n",
    "emp_schema = [\"id\", \"name\", \"date\", \"time\"]\n",
    "# Create a DataFrame using the DataFrame API\n",
    "empls_df = spark.createDataFrame(data=empls_data, schema=emp_schema)\n",
    "# Show the DataFrame\n",
    "empls_df.show(truncate=False)\n",
    "#show the schema of the DataFrame\n",
    "empls_df.printSchema()\n",
    "#show the total number of rows in the DataFrame\n",
    "print(\"Total number of rows in the DataFrame:\", empls_df.count())\n",
    "##0.Create a Window specification\n",
    "window_spec = Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())\n",
    "# Create a new column with the rank of each row within its department\n",
    "e_window_df = e_df.withColumn(\"row_number\", row_number().over(window_spec)).withColumn(\"rank\", rank().over(window_spec)).withColumn(\"dense_rank\", dense_rank().over(window_spec)).withColumn(\"ntile\", ntile(3).over(window_spec))\n",
    "#show the DataFrame\n",
    "e_window_df.show(truncate=False)\n",
    "\n",
    "##1.Create a new column with the percentage of sales of each product in each month\n",
    "# Step 1: Define the lag window (ordered by month per product)\n",
    "#window_spec_lag = Window.partitionBy(\"product_id\").orderBy(\"sales_month\")\n",
    "# Step 2: Get previous month's sales\n",
    "# product_previous_df = product_df.withColumn(\"previous_month_sales\",lag(\"sales\").over(window_spec_lag))\n",
    "# Step 3: Calculate percentage gain/loss (handle nulls safely)\n",
    "#per_loss_gain_df = product_previous_df.withColumn(\"percentage_loss_gain\",round(((col(\"sales\") - col(\"previous_month_sales\")) / col(\"previous_month_sales\")) * 100,2)\n",
    "# Show result\n",
    "#per_loss_gain_df.select(\"product_id\", \"sales_month\", \"sales\", \"previous_month_sales\", \"percentage_loss_gain\").show()\n",
    "\n",
    "\n",
    "##2.Create a new column with the percentage of sales of each product in each month\n",
    "# Step 1: Truncate date to month level\n",
    "#product_df = product_df.withColumn(\"sales_month\", trunc(\"sales_date\", \"month\"))\n",
    "# Step 2: Define window partitioned by product_id and ordered by month\n",
    "#window_spec = Window.partitionBy(\"product_id\").orderBy(\"sales_month\")\n",
    "# Step 3: Calculate monthly sum of sales for each product\n",
    "#sum_sales_df = product_df.withColumn(\"sum_sales\", sum(col(\"sales\")).over(window_spec))\n",
    "# Step 4: Calculate percentage\n",
    "#per_sales_each_month_df = sum_sales_df.withColumn(\"percentage_sales_each_month\",(col(\"sales\") / col(\"sum_sales\") * 100).cast(\"double\"))\n",
    "# Optional: Round the percentage to 2 decimals\n",
    "#per_sales_each_month_df = per_sales_each_month_df.withColumn(\"percentage_sales_each_month\", round(\"percentage_sales_each_month\", 2))\n",
    "# Show final result\n",
    "#per_sales_each_month_df.select(\"product_id\", \"sales_month\", \"sales\", \"sum_sales\", \"percentage_sales_each_month\").show()\n",
    "\n",
    "##3.Create a new column with the first and latest sales & to find the difference between the sales of each product from the first and last month sales\n",
    "#Ensure sales_date is of DateType\n",
    "#sales_df = sales_df.withColumn(\"sales_date\", to_date(col(\"sales_date\")))\n",
    "# Define window partitioned by product and ordered by sales_date\n",
    "#w = Window.partitionBy(\"product_id\").orderBy(\"sales_date\").rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "# Add first and last sales for each product\n",
    "#sales_df_with_extremes = sales_df.withColumn(\"first_sale\", first(\"sales_amount\").over(w)).withColumn(\"last_sale\", last(\"sales_amount\").over(w))\n",
    "# Calculate difference\n",
    "#sales_df_final = sales_df_with_extremes.withColumn(\"sales_difference\", col(\"last_sale\") - col(\"first_sale\"))\n",
    "# Show only unique result per product (optional)\n",
    "#sales_df_final.select(\"product_id\", \"first_sale\", \"last_sale\", \"sales_difference\").distinct().show()\n",
    "\n",
    "##4.send an email to all employees who have not completed compulsory 8 hour office work when they are in the office\n",
    "\n",
    "##5.find out the performance of the sales based on the last three months average sales of each product\n",
    "# Ensure the sales_date column is in DateType\n",
    "#sales_df = sales_df.withColumn(\"sales_date\", to_date(col(\"sales_date\")))\n",
    "# Define a window partitioned by product_id and ordered by date\n",
    "#window_spec = Window.partitionBy(\"product_id\").orderBy(col(\"sales_date\")).rowsBetween(-90, -1)\n",
    "# Calculate 3-month rolling average (excluding the current row)\n",
    "#sales_df_with_avg = sales_df.withColumn(\"avg_sales_last_3_months\",avg(\"sales_amount\").over(window_spec))\n",
    "# Calculate performance compared to average\n",
    "#sales_df_with_perf = sales_df_with_avg.withColumn(\"performance_vs_avg\",when(col(\"avg_sales_last_3_months\").isNull(), None).when(col(\"sales_amount\") > col(\"avg_sales_last_3_months\"), \"Above Average\").when(col(\"sales_amount\") < col(\"avg_sales_last_3_months\"), \"Below Average\").otherwise(\"Average\"))\n",
    "#.select(\"product_id\", \"sales_date\", \"sales_amount\", \"avg_sales_last_3_months\", \"performance_vs_avg\").show()\n",
    "#last_3_months_avg_sales_df = product_df.withColumn(\"last_3_months_avg_sales\", avg(col(\"sales\")).over(window_spec_agg))\n",
    "#STEP 9.11:Transformation in PySpark:Using dataframe API-Nested Json Flattening\n",
    "\n",
    "# Read the JSON file into a DataFrame\n",
    "# 1. Define your JSON string\n",
    "nested_json_str = \"\"\"\n",
    "{\n",
    "  \"restaurants\": [\n",
    "    { \"restaurant\": { \"R\": { \"res_id\": \"1001\" } } },\n",
    "    { \"restaurant\": { \"R\": { \"res_id\": \"1002\" } } }\n",
    "  ]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Parse JSON string to dict\n",
    "data_dict = json.loads(nested_json_str)\n",
    "\n",
    "# Create DataFrame from dict\n",
    "nested_json_df = spark.createDataFrame([data_dict])\n",
    "\n",
    "# 4. Show the DataFrame and Schema\n",
    "print(\"üìã Initial Data Preview:\")\n",
    "nested_json_df.show(truncate=False)\n",
    "nested_json_df.printSchema()\n",
    "print(\"Total number of rows:\", nested_json_df.count())\n",
    "\n",
    "# 5. Flattening the nested JSON\n",
    "# Step A: Explode the 'restaurants' array into new rows\n",
    "nested_json_df_0 = nested_json_df.select(\"*\", explode(col(\"restaurants\")).alias(\"new_restaurant\"))\n",
    "\n",
    "# Step B: Drop the original array column\n",
    "nested_json_df_1 = nested_json_df_0.drop(\"restaurants\")\n",
    "\n",
    "# Step C: Select the deep nested ID (new_restaurant -> restaurant -> R -> res_id)\n",
    "nested_json_df_2 = nested_json_df_1.select(\"new_restaurant.restaurant.R.res_id\")\n",
    "\n",
    "# 6. Final Result\n",
    "print(\"‚ú® Flattened Result (res_id only):\")\n",
    "nested_json_df_2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937d6b7a",
   "metadata": {},
   "source": [
    "FINAL STEP ‚Äî Copy Results BACK to datasets/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fed56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# STEP 3: OUTBOUND BRIDGE (FOR VS CODE SYNC)\n",
    "# =====================================================\n",
    "\n",
    "# 1. Define Paths (Absolute paths are safer for shutil)\n",
    "# current_dir is usually /Workspace/Users/.../dev/files/scripts\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "workspace_output_target = os.path.join(parent_dir, \"datasets\", \"output\")\n",
    "workspace_other_target = os.path.join(parent_dir, \"datasets\", \"other\")\n",
    "\n",
    "def sync_back_to_workspace(volume_src, workspace_dest):\n",
    "    # Check if Volume folder exists locally on the driver mount\n",
    "    if os.path.exists(volume_src):\n",
    "        try:\n",
    "            # Clear the old workspace destination if it exists to avoid 'File exists' errors\n",
    "            if os.path.exists(workspace_dest):\n",
    "                shutil.rmtree(workspace_dest)\n",
    "            \n",
    "            # Create the destination folder\n",
    "            os.makedirs(os.path.dirname(workspace_dest), exist_ok=True)\n",
    "            \n",
    "            # SHUTIL COPY: Bridges Volume mount -> Workspace\n",
    "            shutil.copytree(volume_src, workspace_dest)\n",
    "            print(f\"üéâ Synced back to Workspace: {workspace_dest}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Shutil Sync failed for {volume_src}: {e}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Source Volume folder not found: {volume_src}\")\n",
    "\n",
    "# 2. Run the sync back\n",
    "sync_back_to_workspace(OUTPUT_VOL, workspace_output_target)\n",
    "sync_back_to_workspace(OTHER_VOL, workspace_other_target)\n",
    "\n",
    "print(\"\\n‚úÖ DONE. You can now run 'databricks bundle sync --pull' in VS Code.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
